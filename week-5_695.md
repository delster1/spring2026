---
tags:
    - eecs695
date:
    - 2026-02-17
---

# EECS 695 - Week 5 

## Lec 1 - Regularizations

Last time:
    - subgradients 
    - subdifferentials 
        - allow for gradient-based optimizations w/ functions not differentiable everywhere
    - used to measure data fit

Regularization goal:
- keep model well-behaved so it can work with unseen data
- our form of regularizaiton keeps the problem space convex - there is a local minimum


### Linear Regression - two methods
- least squares - minimizes square prediction error
- ridge regression - adds a second term - l2 norm Regularization along with least squared
    - this penalizes models for having large weight vectors, encouraging more stable models
    - reduces model complexity
    - $\test{Least Squares } + \gamma \norm{w}\_2^2$
        - loss increases with a normalized weight and some scale gamma
- this has a closed form, allowing us to do this math quickly! - analytical solution
    - introduces a regularization term (l2norm) which reduces the impact of changing weights
    - this solution has problems for big input spaces!

A more common approach - finding an objective function 
- conjugate gradient - minimizes a convex objective 
    - takes a sequence of specific directions
    - creates outout from this
    - allows O(smth) iterations for epsilon prec

### **why use l2 regularization?** 
1. It's easier to optimize!
    - regularization doesn't just change the complexity of a solution, but the shape of the problem
    - we make features of the problem that were previously expensive to access easier to access?
        - makes it easier to optimize!
        - CG converges faster as gamma increases!

2. Better generalization
    - Least Squares has better training error, it is optimized for this! 
        - low training error != good performance on unseen data
    - ridge regression makes better predictions on test set - bias-variance decomposition
        - shrinking weights stabilizes solution
        - makes model less sensitive to variations and such

### L1 Regularization
L1 regularization can lead to some coefficients being 0 - some input data may not matter!
    - the only difference is, L1 forces the norm of the weight vector to be less than t

**motivations** 
1. output feature y can be inddependent of some of the input space d
2. if d >> n, linear models are likely to overfit

EX: Genomic data to predict disease 
- d - huge - humans have 20k protien coding genes 
- n - smaller - low number of diseases to select from

**goals:** 
1. Select only features relevant to y
2. Avoid overfitting with big d small n

### ERM Regularizaiton
Regularized Emperical Risk Minimization 

loss fn
- calculates loss as a function of $w, x_i, y_i$
- can use linear or logistic regression here

regularization 
- function of $w, \gamma$
- can use l1, l2, elastic regularization

## L2 - Neuralnet Basics
Goated example - MNIST dataset

I got distracted...

### Gradient & Backpropogation
- applies training rules layer-by-layer using gradient-based metrics 

we want to build an optimization model - minimize prediction loss for all training samples
- we can use Stochastic Gradient Descent!

w^(n)   - weight from layer n 
x       - input feature 
z       - output - weight * input

**How to compute:**
- we know L = Loss(f(x_j, y_j))
- we want to compute the gradient with respect to the weights 
    - partial derivative of loss over weight and chosen outputs from model
        - these controlled by activation functions - ReLU
    - weight and chosen outputs is one term - $z^{(3)}$
    - $z^{(3)}$ is a function of a softmax of outouts and $W^{(2)}$


